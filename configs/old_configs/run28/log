Loaded configuration from  ./configs/config.json

{'batch_size': 32,
 'cosine_annealing': False,
 'desc': 'bninception external data',
 'drop_rate': 0,
 'epochs': 200,
 'exp_name': 'run28',
 'external_data': True,
 'imsize': 512,
 'lr': 0.001,
 'lr_patience': 6,
 'lr_scale': 0.1,
 'model_name': 'bninception',
 'num_channels': 3,
 'num_workers': 6,
 'preload_data': False,
 'pretrained': True,
 'reduce_lr_plateau': True,
 'test_size': 0.2}

Using BN Inception
Loading weights...
Training ...
Saving to  ./model_weights/best_bninception_run28.pth

Epoch 0/200
B: 2783/2783 | Loss: 0.7341  | ETA:    0s
Avg Train Loss: 1.181
B: 195/195 | ETA:    0s
Avg Eval Loss: 1.19, Avg Eval Macro F1: 0.04923, Avg Eval Acc. 0.9479
Best val loss achieved. loss = 1.1903.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1269s

Epoch 1/200
B: 2783/2783 | Loss: 0.6273  | ETA:    0s
Avg Train Loss: 0.8237
B: 195/195 | ETA:    0s
Avg Eval Loss: 1.015, Avg Eval Macro F1: 0.1567, Avg Eval Acc. 0.9525
Best val loss achieved. loss = 1.0154.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1259s

Epoch 2/200
B: 2783/2783 | Loss: 0.7044  | ETA:    0s
Avg Train Loss: 0.6948
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.9149, Avg Eval Macro F1: 0.2384, Avg Eval Acc. 0.9578
Best val loss achieved. loss = 0.9149.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1260s

Epoch 3/200
B: 2783/2783 | Loss: 0.4582  | ETA:    0s
Avg Train Loss: 0.6267
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.8363, Avg Eval Macro F1: 0.2837, Avg Eval Acc. 0.9606
Best val loss achieved. loss = 0.8363.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1261s

Epoch 4/200
B: 2783/2783 | Loss: 0.4944  | ETA:    0s
Avg Train Loss: 0.5876
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7985, Avg Eval Macro F1: 0.3623, Avg Eval Acc. 0.9633
Best val loss achieved. loss = 0.7985.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1260s

Epoch 5/200
B: 2783/2783 | Loss: 0.7245  | ETA:    0s
Avg Train Loss: 0.5604
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7466, Avg Eval Macro F1: 0.4047, Avg Eval Acc. 0.9649
Best val loss achieved. loss = 0.7466.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1262s

Epoch 6/200
B: 2783/2783 | Loss: 0.6422  | ETA:    0s
Avg Train Loss: 0.5388
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.737, Avg Eval Macro F1: 0.3746, Avg Eval Acc. 0.9656
Best val loss achieved. loss = 0.7370.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1271s

Epoch 7/200
B: 2783/2783 | Loss: 0.5640  | ETA:    0s
Avg Train Loss: 0.523
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7548, Avg Eval Macro F1: 0.3766, Avg Eval Acc. 0.9648
Time: 1276s

Epoch 8/200
^CProcess Process-98:.5172  | ETA: 1194s
Process Process-99:
Process Process-100:
Process Process-102:
Process Process-97:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 96, in _worker_loop
    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 96, in _worker_loop
    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py", line 104, in get
    if not self._poll(timeout):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 414, in _poll
    r = wait([self], timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 920, in wait
    ready = selector.select(timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 920, in wait
    ready = selector.select(timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
KeyboardInterrupt
Process Process-101:
Traceback (most recent call last):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 81, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/litemax/kaggle/Human-Protein-Atlas/utils/dataloader.py", line 243, in __getitem__
    image = cv2.imread(imagepath, cv2.IMREAD_COLOR)
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 81, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/litemax/kaggle/Human-Protein-Atlas/utils/dataloader.py", line 246, in __getitem__
    image = self.transformer(image=image)['image']
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 81, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 63, in __call__
    data = self.run_transforms_if_needed(need_to_run, data)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 78, in run_transforms_if_needed
    data = t(**data)
  File "/home/litemax/kaggle/Human-Protein-Atlas/utils/dataloader.py", line 246, in __getitem__
    image = self.transformer(image=image)['image']
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 63, in __call__
    data = self.run_transforms_if_needed(need_to_run, data)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/transforms_interface.py", line 28, in __call__
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/transforms.py", line 610, in apply
    return F.normalize(image, self.mean, self.std, self.max_pixel_value)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/functional.py", line 87, in normalize
    img = cv2.subtract(img, np.ones_like(img) * np.asarray(mean, dtype=np.float32))
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 78, in run_transforms_if_needed
    data = t(**data)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/transforms_interface.py", line 28, in __call__
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/transforms.py", line 610, in apply
    return F.normalize(image, self.mean, self.std, self.max_pixel_value)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/functional.py", line 88, in normalize
    img = cv2.divide(img, np.ones_like(img) * np.asarray(std, dtype=np.float32))
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 106, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 81, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/litemax/kaggle/Human-Protein-Atlas/utils/dataloader.py", line 246, in __getitem__
    image = self.transformer(image=image)['image']
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 63, in __call__
    data = self.run_transforms_if_needed(need_to_run, data)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/composition.py", line 78, in run_transforms_if_needed
    data = t(**data)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/transforms_interface.py", line 28, in __call__
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/transforms.py", line 610, in apply
    return F.normalize(image, self.mean, self.std, self.max_pixel_value)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/functional.py", line 87, in normalize
    img = cv2.subtract(img, np.ones_like(img) * np.asarray(mean, dtype=np.float32))
KeyboardInterrupt
Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f19a73bd158>
Traceback (most recent call last):
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 390, in _shutdown_workers
    self.worker_result_queue.put(None)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py", line 364, in put
    self._writer.send_bytes(obj)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 404, in _send_bytes
    self._send(header + buf)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
  File "/home/litemax/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 227, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 12240) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
B: 180/2783 | Loss: 0.7511  | ETA: 1194s

Generate submission while the GPU is still hot from training? [Y/n]: n
(pytorch) litemax@litemax-hub:~/kaggle/Human-Protein-Atlas$ python train.py --resume
  File "train.py", line 256
    else:
       ^
SyntaxError: invalid syntax
(pytorch) litemax@litemax-hub:~/kaggle/Human-Protein-Atlas$ python train.py --resume
Loaded configuration from  ./configs/config.json

{'batch_size': 32,
 'cosine_annealing': False,
 'desc': 'bninception external data',
 'drop_rate': 0,
 'epochs': 200,
 'exp_name': 'run28',
 'external_data': True,
 'imsize': 512,
 'lr': 0.001,
 'lr_patience': 6,
 'lr_scale': 0.1,
 'model_name': 'bninception',
 'num_channels': 3,
 'num_workers': 6,
 'preload_data': False,
 'pretrained': True,
 'reduce_lr_plateau': True,
 'test_size': 0.2}

Using BN Inception
Loading weights...
Resuming from checkpoint './model_weights/best_bninception_run28.pth'
Loaded checkpoint './model_weights/best_bninception_run28.pth' (epoch 6)
Training ...
Saving to  ./model_weights/best_bninception_run28.pth

Epoch 0/200
B: 2784/2784 | Loss: 0.8395  | ETA:    0s
Avg Train Loss: 0.5241
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7128, Avg Eval Macro F1: 0.3961, Avg Eval Acc. 0.9664
Best val loss achieved. loss = 0.7128.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1256s

Epoch 1/200
B: 2784/2784 | Loss: 1.2152  | ETA:    0s
Avg Train Loss: 0.5125
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7445, Avg Eval Macro F1: 0.3995, Avg Eval Acc. 0.9648
Time: 1242s

Epoch 2/200
B: 2784/2784 | Loss: 0.6892  | ETA:    0s
Avg Train Loss: 0.5037
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.8021, Avg Eval Macro F1: 0.355, Avg Eval Acc. 0.9638
Time: 1240s

Epoch 3/200
B: 2784/2784 | Loss: 0.3216  | ETA:    0s
Avg Train Loss: 0.491
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7056, Avg Eval Macro F1: 0.4433, Avg Eval Acc. 0.9665
Best val loss achieved. loss = 0.7056.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1241s

Epoch 4/200
B: 2784/2784 | Loss: 0.1530  | ETA:    0s
Avg Train Loss: 0.4843
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6943, Avg Eval Macro F1: 0.4923, Avg Eval Acc. 0.9673
Best val loss achieved. loss = 0.6943.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 5/200
B: 2784/2784 | Loss: 0.7003  | ETA:    0s
Avg Train Loss: 0.476
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6809, Avg Eval Macro F1: 0.4711, Avg Eval Acc. 0.968
Best val loss achieved. loss = 0.6809.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1241s

Epoch 6/200
B: 2784/2784 | Loss: 0.1444  | ETA:    0s
Avg Train Loss: 0.4688
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6895, Avg Eval Macro F1: 0.4626, Avg Eval Acc. 0.9681
Time: 1237s

Epoch 7/200
B: 2784/2784 | Loss: 0.8123  | ETA:    0s
Avg Train Loss: 0.4634
B: 195/195 | ETA:    0s
Avg Eval Loss: 1.226, Avg Eval Macro F1: 0.4128, Avg Eval Acc. 0.9397
Time: 1248s

Epoch 8/200
B: 2784/2784 | Loss: 2.2366  | ETA:    0s
Avg Train Loss: 0.4601
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7791, Avg Eval Macro F1: 0.5144, Avg Eval Acc. 0.9636
Time: 1239s

Epoch 9/200
B: 2784/2784 | Loss: 0.1075  | ETA:    0s
Avg Train Loss: 0.4538
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6531, Avg Eval Macro F1: 0.5161, Avg Eval Acc. 0.97
Best val loss achieved. loss = 0.6531.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1238s

Epoch 10/200
B: 2784/2784 | Loss: 1.5258  | ETA:    0s
Avg Train Loss: 0.4475
B: 195/195 | ETA:    0s
Avg Eval Loss: 1.825, Avg Eval Macro F1: 0.3703, Avg Eval Acc. 0.8975
Time: 1239s

Epoch 11/200
B: 2784/2784 | Loss: 0.6835  | ETA:    0s
Avg Train Loss: 0.4445
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6351, Avg Eval Macro F1: 0.5458, Avg Eval Acc. 0.9702
Best val loss achieved. loss = 0.6351.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 12/200
B: 2784/2784 | Loss: 0.6463  | ETA:    0s
Avg Train Loss: 0.438
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6325, Avg Eval Macro F1: 0.5567, Avg Eval Acc. 0.9704
Best val loss achieved. loss = 0.6325.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1241s

Epoch 13/200
B: 2784/2784 | Loss: 1.0801  | ETA:    0s
Avg Train Loss: 0.4333
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.7645, Avg Eval Macro F1: 0.4154, Avg Eval Acc. 0.9656
Time: 1239s

Epoch 14/200
B: 2784/2784 | Loss: 1.3101  | ETA:    0s
Avg Train Loss: 0.431
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6464, Avg Eval Macro F1: 0.5301, Avg Eval Acc. 0.9703
Time: 1241s

Epoch 15/200
B: 2784/2784 | Loss: 2.3384  | ETA:    0s
Avg Train Loss: 0.426
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6151, Avg Eval Macro F1: 0.5723, Avg Eval Acc. 0.9714
Best val loss achieved. loss = 0.6151.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 16/200
B: 2784/2784 | Loss: 2.4547  | ETA:    0s
Avg Train Loss: 0.4234
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6151, Avg Eval Macro F1: 0.5654, Avg Eval Acc. 0.9711
Time: 1238s

Epoch 17/200
B: 2784/2784 | Loss: 0.6215  | ETA:    0s
Avg Train Loss: 0.4174
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6442, Avg Eval Macro F1: 0.5666, Avg Eval Acc. 0.9701
Time: 1239s

Epoch 18/200
B: 2784/2784 | Loss: 0.9764  | ETA:    0s
Avg Train Loss: 0.4168
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5982, Avg Eval Macro F1: 0.5936, Avg Eval Acc. 0.9722
Best val loss achieved. loss = 0.5982.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 19/200
B: 2784/2784 | Loss: 0.2637  | ETA:    0s
Avg Train Loss: 0.4088
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5994, Avg Eval Macro F1: 0.5743, Avg Eval Acc. 0.9718
Time: 1239s

Epoch 20/200
B: 2784/2784 | Loss: 1.2496  | ETA:    0s
Avg Train Loss: 0.4045
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6026, Avg Eval Macro F1: 0.5556, Avg Eval Acc. 0.9713
Time: 1239s

Epoch 21/200
B: 2784/2784 | Loss: 1.1278  | ETA:    0s
Avg Train Loss: 0.4013
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6178, Avg Eval Macro F1: 0.5707, Avg Eval Acc. 0.9713
Time: 1240s

Epoch 22/200
B: 2784/2784 | Loss: 0.5375  | ETA:    0s
Avg Train Loss: 0.3974
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6212, Avg Eval Macro F1: 0.5153, Avg Eval Acc. 0.9714
Time: 1240s

Epoch 23/200
B: 2784/2784 | Loss: 1.2222  | ETA:    0s
Avg Train Loss: 0.3945
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6288, Avg Eval Macro F1: 0.585, Avg Eval Acc. 0.971
Time: 1242s

Epoch 24/200
B: 2784/2784 | Loss: 1.8963  | ETA:    0s
Avg Train Loss: 0.3901
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6242, Avg Eval Macro F1: 0.5623, Avg Eval Acc. 0.9713
Time: 1241s

Epoch 25/200
B: 2784/2784 | Loss: 1.9735  | ETA:    0s
Avg Train Loss: 0.3877
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.6348, Avg Eval Macro F1: 0.5666, Avg Eval Acc. 0.9705
Epoch    25: reducing learning rate of group 0 to 1.0000e-04.
Time: 1239s

Epoch 26/200
B: 2784/2784 | Loss: 0.2955  | ETA:    0s
Avg Train Loss: 0.3446
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5721, Avg Eval Macro F1: 0.6133, Avg Eval Acc. 0.9732
Best val loss achieved. loss = 0.5721.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 27/200
B: 2784/2784 | Loss: 0.4281  | ETA:    0s
Avg Train Loss: 0.3332
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5594, Avg Eval Macro F1: 0.6538, Avg Eval Acc. 0.9748
Best val loss achieved. loss = 0.5594.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 28/200
B: 2784/2784 | Loss: 0.1295  | ETA:    0s
Avg Train Loss: 0.3304
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5464, Avg Eval Macro F1: 0.6487, Avg Eval Acc. 0.9746
Best val loss achieved. loss = 0.5464.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1240s

Epoch 29/200
B: 2784/2784 | Loss: 0.2628  | ETA:    0s
Avg Train Loss: 0.3246
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5545, Avg Eval Macro F1: 0.6424, Avg Eval Acc. 0.9745
Time: 1241s

Epoch 30/200
B: 2784/2784 | Loss: 0.0460  | ETA:    0s
Avg Train Loss: 0.3244
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5472, Avg Eval Macro F1: 0.6442, Avg Eval Acc. 0.9745
Time: 1241s

Epoch 31/200
B: 2784/2784 | Loss: 1.6615  | ETA:    0s
Avg Train Loss: 0.3224
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5532, Avg Eval Macro F1: 0.6342, Avg Eval Acc. 0.9746
Time: 1239s

Epoch 32/200
B: 2784/2784 | Loss: 0.5748  | ETA:    0s
Avg Train Loss: 0.3194
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5604, Avg Eval Macro F1: 0.6559, Avg Eval Acc. 0.9748
Time: 1240s

Epoch 33/200
B: 2784/2784 | Loss: 3.7014  | ETA:    0s
Avg Train Loss: 0.3187
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5725, Avg Eval Macro F1: 0.6514, Avg Eval Acc. 0.9738
Time: 1240s

Epoch 34/200
B: 2784/2784 | Loss: 0.9695  | ETA:    0s
Avg Train Loss: 0.3152
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.569, Avg Eval Macro F1: 0.6531, Avg Eval Acc. 0.9745
Time: 1240s

Epoch 35/200
B: 2784/2784 | Loss: 1.3266  | ETA:    0s
Avg Train Loss: 0.3132
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5679, Avg Eval Macro F1: 0.6513, Avg Eval Acc. 0.9747
Epoch    35: reducing learning rate of group 0 to 1.0000e-05.
Time: 1241s

Epoch 36/200
B: 2784/2784 | Loss: 1.2868  | ETA:    0s
Avg Train Loss: 0.3079
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5583, Avg Eval Macro F1: 0.6653, Avg Eval Acc. 0.9746
Time: 1242s

Epoch 37/200
B: 2784/2784 | Loss: 2.4290  | ETA:    0s
Avg Train Loss: 0.308
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5572, Avg Eval Macro F1: 0.6204, Avg Eval Acc. 0.9744
Time: 1241s

Epoch 38/200
B: 2784/2784 | Loss: 1.4131  | ETA:    0s
Avg Train Loss: 0.3071
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5504, Avg Eval Macro F1: 0.6392, Avg Eval Acc. 0.975
Time: 1239s

Epoch 39/200
B: 2784/2784 | Loss: 3.5905  | ETA:    0s
Avg Train Loss: 0.3076
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5555, Avg Eval Macro F1: 0.6771, Avg Eval Acc. 0.9746
Time: 1242s

Epoch 40/200
B: 2784/2784 | Loss: 1.3466  | ETA:    0s
Avg Train Loss: 0.3061
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.614, Avg Eval Macro F1: 0.5772, Avg Eval Acc. 0.9729
Time: 1241s

Epoch 41/200
B: 2784/2784 | Loss: 1.1594  | ETA:    0s
Avg Train Loss: 0.3047
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5994, Avg Eval Macro F1: 0.6411, Avg Eval Acc. 0.9724
Time: 1243s

Epoch 42/200
B: 2784/2784 | Loss: 1.3916  | ETA:    0s
Avg Train Loss: 0.3061
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.56, Avg Eval Macro F1: 0.6676, Avg Eval Acc. 0.9747
Epoch    42: reducing learning rate of group 0 to 1.0000e-06.
Time: 1239s

Epoch 43/200
B: 2784/2784 | Loss: 0.1027  | ETA:    0s
Avg Train Loss: 0.3045
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5832, Avg Eval Macro F1: 0.6897, Avg Eval Acc. 0.9729
Time: 1256s

Epoch 44/200
B: 2784/2784 | Loss: 0.6404  | ETA:    0s
Avg Train Loss: 0.3055
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5649, Avg Eval Macro F1: 0.6461, Avg Eval Acc. 0.9747
Time: 1241s

Epoch 45/200
B: 2784/2784 | Loss: 0.7597  | ETA:    0s
Avg Train Loss: 0.3053
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5491, Avg Eval Macro F1: 0.6518, Avg Eval Acc. 0.975
Time: 1241s

Epoch 46/200
B: 2784/2784 | Loss: 0.1296  | ETA:    0s
Avg Train Loss: 0.3055
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5462, Avg Eval Macro F1: 0.6762, Avg Eval Acc. 0.9747
Best val loss achieved. loss = 0.5462.  Saving model to  ./model_weights/best_bninception_run28.pth
Time: 1242s

Epoch 47/200
B: 2784/2784 | Loss: 1.9925  | ETA:    0s
Avg Train Loss: 0.3045
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5533, Avg Eval Macro F1: 0.6415, Avg Eval Acc. 0.9748
Time: 1242s

Epoch 48/200
B: 2784/2784 | Loss: 0.8070  | ETA:    0s
Avg Train Loss: 0.3043
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5581, Avg Eval Macro F1: 0.6676, Avg Eval Acc. 0.975
Time: 1244s

Epoch 49/200
B: 2784/2784 | Loss: 1.2967  | ETA:    0s
Avg Train Loss: 0.3036
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5611, Avg Eval Macro F1: 0.6308, Avg Eval Acc. 0.9743
Time: 1244s

Epoch 50/200
B: 2784/2784 | Loss: 1.0486  | ETA:    0s
Avg Train Loss: 0.3046
B: 195/195 | ETA:    0s
Avg Eval Loss: 0.5561, Avg Eval Macro F1: 0.6653, Avg Eval Acc. 0.9745
Time: 1243s
